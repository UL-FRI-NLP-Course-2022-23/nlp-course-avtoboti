{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 822kB/s]                     \n",
      "2023-05-17 19:08:59 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/default.zip: 100%|██████████| 594M/594M [00:13<00:00, 44.9MB/s] \n",
      "2023-05-17 19:09:29 INFO: Finished downloading models and saved to ../models/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "from main import read_file, get_sentences\n",
    "import stanza\n",
    "\n",
    "stanza.download(\"en\", model_dir=\"../models/stanza_resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:10:19 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 29.5MB/s]                    \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/tokenize/combined.pt: 100%|██████████| 647k/647k [00:00<00:00, 40.0MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/pos/combined.pt: 100%|██████████| 38.5M/38.5M [00:00<00:00, 83.6MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/lemma/combined.pt: 100%|██████████| 4.17M/4.17M [00:00<00:00, 87.5MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/ner/ontonotes.pt: 100%|██████████| 46.2M/46.2M [00:00<00:00, 94.2MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/pretrain/combined.pt: 100%|██████████| 107M/107M [00:01<00:00, 92.1MB/s] \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/pretrain/fasttextcrawl.pt: 100%|██████████| 123M/123M [00:01<00:00, 100MB/s]  \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/backward_charlm/1billion.pt: 100%|██████████| 22.7M/22.7M [00:00<00:00, 91.6MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/forward_charlm/1billion.pt: 100%|██████████| 22.7M/22.7M [00:00<00:00, 95.2MB/s]\n",
      "2023-05-17 19:10:28 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-05-17 19:10:28 INFO: Using device: cpu\n",
      "2023-05-17 19:10:28 INFO: Loading: tokenize\n",
      "2023-05-17 19:10:28 INFO: Loading: pos\n",
      "2023-05-17 19:10:28 INFO: Loading: lemma\n",
      "2023-05-17 19:10:28 INFO: Loading: ner\n",
      "2023-05-17 19:10:29 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "henry_red = read_file(\"../data/english_short_stories/LeiningenVstheAnts.txt\")\n",
    "\n",
    "preprocess = stanza.Pipeline(\"en\", processors=\"tokenize, ner, pos, lemma\")\n",
    "\n",
    "doc = preprocess(henry_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences(doc, lambda x: x.upos != \"PUNCT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Leiningen': 51,\n",
       " 'Carl': 1,\n",
       " 'Stephenson': 1,\n",
       " 'Hood': 1,\n",
       " 'Dullards': 1,\n",
       " 'Pell': 1,\n",
       " '-': 1,\n",
       " 'mell': 1,\n",
       " 'Dawn': 1,\n",
       " 'Macbeth': 1,\n",
       " 'Birnam': 1,\n",
       " 'Wood': 1,\n",
       " 'Scot': 1,\n",
       " 'Dante': 1,\n",
       " 'Spry': 1,\n",
       " 'Swift': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import get_named_entities\n",
    "named_entities = get_named_entities(doc)\n",
    "\n",
    "names = []\n",
    "for ne in named_entities:\n",
    "    for named_entity in ne:\n",
    "        names.append(named_entity.to_dict()[0]['lemma'])\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "name_frequencies = Counter(names)\n",
    "name_frequencies = {k:v for (k,v) in sorted(name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "name_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 819/819 [00:00<00:00, 697kB/s]\n",
      "Downloading: 100%|██████████| 393/393 [00:00<00:00, 400kB/s]\n",
      "Downloading: 100%|██████████| 780k/780k [00:00<00:00, 1.92MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 1.46MB/s]\n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 2.61MB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 196kB/s]\n",
      "Downloading: 100%|██████████| 345M/345M [00:09<00:00, 39.4MB/s] \n",
      "05/17/2023 19:15:40 - INFO - \t missing_keys: []\n",
      "05/17/2023 19:15:40 - INFO - \t unexpected_keys: []\n",
      "05/17/2023 19:15:40 - INFO - \t mismatched_keys: []\n",
      "05/17/2023 19:15:40 - INFO - \t error_msgs: []\n",
      "05/17/2023 19:15:40 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastcoref.spacy_component.spacy_component.FastCorefResolver at 0x7f8d7cbcca60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from fastcoref import spacy_component\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('fastcoref')\n",
    "\n",
    "# This is better model\n",
    "# nlp.add_pipe(\"fastcoref\", config={'model_architecture': 'LingMessCoref', 'model_path': 'biu-nlp/lingmess-coref', 'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 21:48:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafcaf569ab345d3ac3903060b8a4780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 21:48:59 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606e449279174248b8ed4dd6edb246b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Leiningen': 303,\n",
       " 'Macbeth': 3,\n",
       " 'Carl': 1,\n",
       " 'Stephenson': 1,\n",
       " 'Hood': 1,\n",
       " 'Pell': 1,\n",
       " '-': 1,\n",
       " 'mell': 1,\n",
       " 'Dawn': 1,\n",
       " 'Birnam': 1,\n",
       " 'Wood': 1,\n",
       " 'Dante': 1,\n",
       " 'Spry': 1,\n",
       " 'Swift': 1}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# henry_red = read_file(\"../data/english_short_stories/Henry_Red_Chief.txt\")\n",
    "\n",
    "# This substitutes pronouns with appropriate Named entities\n",
    "coref_doc = nlp(henry_red, component_cfg={'fastcoref': {'resolve_text': True}})\n",
    "resolved_text = coref_doc._.resolved_text\n",
    "\n",
    "# Again perform NER on new text to get all the positions of the NE.\n",
    "ne_doc = preprocess(resolved_text)\n",
    "corrected_sentences = get_sentences(ne_doc, lambda x: x.upos != \"PUNCT\")\n",
    "total_named_entities = get_named_entities(ne_doc)\n",
    "total_names = []\n",
    "for ne in total_named_entities:\n",
    "    for named_entity in ne:\n",
    "        total_names.append(named_entity.text)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "total_name_frequencies = Counter(total_names)\n",
    "total_name_frequencies = {k:v for (k,v) in sorted(total_name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "keys_to_delete = []\n",
    "for name in total_name_frequencies:\n",
    "    for name2 in total_name_frequencies:\n",
    "        if name == name2[:-2]:\n",
    "            total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "            keys_to_delete.append(name2)\n",
    "        if name in name2 and name != name2:\n",
    "            total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "            keys_to_delete.append(name2)\n",
    "for k in keys_to_delete:\n",
    "    if k in total_name_frequencies:\n",
    "        total_name_frequencies.pop(k) \n",
    "total_name_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import get_named_entities\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from fastcoref import spacy_component\n",
    "\n",
    "def get_most_mentioned_characters(raw_text):\n",
    "    # tokenize, pos, lemma, ner\n",
    "    doc_ne = preprocess(raw_text)\n",
    "    # extract only NE with PERSON tag\n",
    "    named_entities = get_named_entities(doc_ne)\n",
    "\n",
    "    # store all character lemmas\n",
    "    names = []\n",
    "    for ne in named_entities:\n",
    "        for named_entity in ne:\n",
    "            names.append(named_entity.to_dict()[0]['lemma'])\n",
    "\n",
    "    # count frequencies of each name and sort them\n",
    "    name_frequencies = Counter(names)\n",
    "    name_frequencies = {k:v for (k,v) in sorted(name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Coreference resolution\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe('fastcoref')\n",
    "\n",
    "    # This substitutes pronouns with appropriate Named entities\n",
    "    doc_coref = nlp(raw_text, component_cfg={'fastcoref': {'resolve_text': True}})\n",
    "    resolved_text = doc_coref._.resolved_text\n",
    "\n",
    "    # Again perform NER on new text to get all the positions of the NE.\n",
    "    doc_ne_pronouns = preprocess(resolved_text)\n",
    "    total_named_entities = get_named_entities(doc_ne_pronouns)\n",
    "    total_names = []\n",
    "    for ne in total_named_entities:\n",
    "        for named_entity in ne:\n",
    "            total_names.append(named_entity.to_dict()[0]['lemma'])\n",
    "\n",
    "    # Again calculate frequencies...\n",
    "    total_name_frequencies = Counter(total_names)\n",
    "    total_name_frequencies = {k:v for (k,v) in sorted(total_name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Join same names. E.g. Bill, Bill's...or Name Surname, Name\n",
    "    keys_to_delete = []\n",
    "    for name in total_name_frequencies:\n",
    "        for name2 in total_name_frequencies:\n",
    "            if name == name2[:-2]:\n",
    "                total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "                keys_to_delete.append(name2)\n",
    "            if name in name2 and name != name2:\n",
    "                total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "                keys_to_delete.append(name2)\n",
    "    for k in keys_to_delete:\n",
    "        if k in total_name_frequencies:\n",
    "            total_name_frequencies.pop(k)\n",
    "    \n",
    "    total_name_frequencies = {k:v for (k,v) in sorted(total_name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return total_name_frequencies\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not read file '../data/english_short_stories/Henry_Red_Chief.txt' using the utf-8 encoding\n",
      "'utf-8' codec can't decode byte 0xd1 in position 123: invalid continuation byte\n",
      "Could not read file '../data/english_short_stories/Henry_Red_Chief.txt' using the ascii encoding\n",
      "'ascii' codec can't decode byte 0xd1 in position 123: ordinal not in range(128)\n",
      "04/25/2023 17:56:57 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:56:57 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:56:57 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:56:57 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:56:57 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:56:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3c1a4882b649cc82a3cca64f445637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:56:58 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136b9a4af14040c6bcf01cad0263af2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:57:32 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:57:32 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:57:32 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:57:32 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:57:32 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:57:32 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7ec880bfcd4a7dbb1498e130a9f2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:57:32 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99a5293eedb46c4aad96456f46d68ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:57:53 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:57:53 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:57:53 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:57:53 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:57:53 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:57:54 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632f700fa4424f059919077ae207d10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:57:54 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee5fa15c3c44c6a964c24d8917ed3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not read file '../data/english_short_stories/Hills Like White Elephants.txt' using the utf-8 encoding\n",
      "'utf-8' codec can't decode byte 0xd1 in position 2447: invalid continuation byte\n",
      "Could not read file '../data/english_short_stories/Hills Like White Elephants.txt' using the ascii encoding\n",
      "'ascii' codec can't decode byte 0xd1 in position 2447: ordinal not in range(128)\n",
      "04/25/2023 17:58:21 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:58:21 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:58:21 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:58:21 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:58:21 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:58:21 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763c9cc071b243f6a25d2c8ed8f17563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:58:21 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0b5dbb277b4934b8ed566fec056ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:58:57 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:58:57 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:58:57 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:58:57 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:58:57 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:58:59 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeee916787c44d9db39bcad60c9530d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:58:59 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3284c2f7e37d47aabaee2224dade4a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not read file '../data/english_short_stories/the_gift_of_the_magi_0_Henry.txt' using the utf-8 encoding\n",
      "'utf-8' codec can't decode byte 0xd2 in position 682: invalid continuation byte\n",
      "Could not read file '../data/english_short_stories/the_gift_of_the_magi_0_Henry.txt' using the ascii encoding\n",
      "'ascii' codec can't decode byte 0xd2 in position 682: ordinal not in range(128)\n",
      "04/25/2023 17:59:43 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:59:43 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:59:43 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:59:43 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:59:43 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:59:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4aea0390bea49629cdb34e1a9d0e2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:59:44 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da310bf6a4a34348bedbb521c25d4419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 18:00:12 - INFO - \t missing_keys: []\n",
      "04/25/2023 18:00:12 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 18:00:12 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 18:00:12 - INFO - \t error_msgs: []\n",
      "04/25/2023 18:00:12 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 18:00:14 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cad26742e84cce9abc4da1bdfe0256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 18:00:15 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad96e7d09bca4187812b512ec190e978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from main import read_file\n",
    "import os\n",
    "\n",
    "characters_by_stories = []\n",
    "\n",
    "for f in os.listdir('../data/english_short_stories/'):\n",
    "    raw_text = read_file(f'../data/english_short_stories/{f}')\n",
    "    name_freqs = get_most_mentioned_characters(raw_text=raw_text)\n",
    "    characters_by_stories.append(name_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_data = json.dumps(characters_by_stories, indent=4)\n",
    "with open('../data/characters_by_stories.json', 'w') as outfile:\n",
    "    outfile.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
