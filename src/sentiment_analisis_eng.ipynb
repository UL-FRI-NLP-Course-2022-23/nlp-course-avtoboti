{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9676285eefe14615b190e2100a9b243f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:51:38 - INFO - \t Downloading default packages for language: en (English) ...\n",
      "04/25/2023 17:51:40 - INFO - \t File exists: ../models/stanza_resources/en/default.zip\n",
      "04/25/2023 17:51:43 - INFO - \t Finished downloading models and saved to ../models/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "from main import read_file, get_sentences\n",
    "import stanza\n",
    "\n",
    "stanza.download(\"en\", model_dir=\"../models/stanza_resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0172a7a6ef98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhenry_red\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/english_short_stories/LeiningenVstheAnts.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tokenize, ner, pos, lemma\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhenry_red\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_file' is not defined"
     ]
    }
   ],
   "source": [
    "henry_red = read_file(\"../data/english_short_stories/LeiningenVstheAnts.txt\")\n",
    "\n",
    "preprocess = stanza.Pipeline(\"en\", processors=\"tokenize, ner, pos, lemma\")\n",
    "\n",
    "doc = preprocess(henry_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences(doc, lambda x: x.upos != \"PUNCT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Leiningen': 51,\n",
       " 'Carl': 1,\n",
       " 'Stephenson': 1,\n",
       " 'Hood': 1,\n",
       " 'Dullards': 1,\n",
       " 'Pell': 1,\n",
       " '-': 1,\n",
       " 'mell': 1,\n",
       " 'Dawn': 1,\n",
       " 'Macbeth': 1,\n",
       " 'Birnam': 1,\n",
       " 'Wood': 1,\n",
       " 'Scot': 1,\n",
       " 'Dante': 1,\n",
       " 'Spry': 1,\n",
       " 'Swift': 1}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import get_named_entities\n",
    "named_entities = get_named_entities(doc)\n",
    "\n",
    "names = []\n",
    "for ne in named_entities:\n",
    "    for named_entity in ne:\n",
    "        names.append(named_entity.to_dict()[0]['lemma'])\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "name_frequencies = Counter(names)\n",
    "name_frequencies = {k:v for (k,v) in sorted(name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "name_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 21:48:53 - INFO - \t missing_keys: []\n",
      "04/24/2023 21:48:53 - INFO - \t unexpected_keys: []\n",
      "04/24/2023 21:48:53 - INFO - \t mismatched_keys: []\n",
      "04/24/2023 21:48:53 - INFO - \t error_msgs: []\n",
      "04/24/2023 21:48:53 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastcoref.spacy_component.spacy_component.FastCorefResolver at 0x14bdeb790>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from fastcoref import spacy_component\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('fastcoref')\n",
    "\n",
    "# This is better model\n",
    "# nlp.add_pipe(\"fastcoref\", config={'model_architecture': 'LingMessCoref', 'model_path': 'biu-nlp/lingmess-coref', 'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 21:48:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafcaf569ab345d3ac3903060b8a4780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 21:48:59 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606e449279174248b8ed4dd6edb246b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Leiningen': 303,\n",
       " 'Macbeth': 3,\n",
       " 'Carl': 1,\n",
       " 'Stephenson': 1,\n",
       " 'Hood': 1,\n",
       " 'Pell': 1,\n",
       " '-': 1,\n",
       " 'mell': 1,\n",
       " 'Dawn': 1,\n",
       " 'Birnam': 1,\n",
       " 'Wood': 1,\n",
       " 'Dante': 1,\n",
       " 'Spry': 1,\n",
       " 'Swift': 1}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# henry_red = read_file(\"../data/english_short_stories/Henry_Red_Chief.txt\")\n",
    "\n",
    "# This substitutes pronouns with appropriate Named entities\n",
    "coref_doc = nlp(henry_red, component_cfg={'fastcoref': {'resolve_text': True}})\n",
    "resolved_text = coref_doc._.resolved_text\n",
    "\n",
    "# Again perform NER on new text to get all the positions of the NE.\n",
    "ne_doc = preprocess(resolved_text)\n",
    "corrected_sentences = get_sentences(ne_doc, lambda x: x.upos != \"PUNCT\")\n",
    "total_named_entities = get_named_entities(ne_doc)\n",
    "total_names = []\n",
    "for ne in total_named_entities:\n",
    "    for named_entity in ne:\n",
    "        total_names.append(named_entity.text)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "total_name_frequencies = Counter(total_names)\n",
    "total_name_frequencies = {k:v for (k,v) in sorted(total_name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "keys_to_delete = []\n",
    "for name in total_name_frequencies:\n",
    "    for name2 in total_name_frequencies:\n",
    "        if name == name2[:-2]:\n",
    "            total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "            keys_to_delete.append(name2)\n",
    "        if name in name2 and name != name2:\n",
    "            total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "            keys_to_delete.append(name2)\n",
    "for k in keys_to_delete:\n",
    "    if k in total_name_frequencies:\n",
    "        total_name_frequencies.pop(k) \n",
    "total_name_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import get_named_entities\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from fastcoref import spacy_component\n",
    "\n",
    "def get_most_mentioned_characters(raw_text):\n",
    "    # tokenize, pos, lemma, ner\n",
    "    doc_ne = preprocess(raw_text)\n",
    "    # extract only NE with PERSON tag\n",
    "    named_entities = get_named_entities(doc_ne)\n",
    "\n",
    "    # store all character lemmas\n",
    "    names = []\n",
    "    for ne in named_entities:\n",
    "        for named_entity in ne:\n",
    "            names.append(named_entity.to_dict()[0]['lemma'])\n",
    "\n",
    "    # count frequencies of each name and sort them\n",
    "    name_frequencies = Counter(names)\n",
    "    name_frequencies = {k:v for (k,v) in sorted(name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Coreference resolution\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe('fastcoref')\n",
    "\n",
    "    # This substitutes pronouns with appropriate Named entities\n",
    "    doc_coref = nlp(raw_text, component_cfg={'fastcoref': {'resolve_text': True}})\n",
    "    resolved_text = doc_coref._.resolved_text\n",
    "\n",
    "    # Again perform NER on new text to get all the positions of the NE.\n",
    "    doc_ne_pronouns = preprocess(resolved_text)\n",
    "    total_named_entities = get_named_entities(doc_ne_pronouns)\n",
    "    total_names = []\n",
    "    for ne in total_named_entities:\n",
    "        for named_entity in ne:\n",
    "            total_names.append(named_entity.to_dict()[0]['lemma'])\n",
    "\n",
    "    # Again calculate frequencies...\n",
    "    total_name_frequencies = Counter(total_names)\n",
    "    total_name_frequencies = {k:v for (k,v) in sorted(total_name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Join same names. E.g. Bill, Bill's...or Name Surname, Name\n",
    "    keys_to_delete = []\n",
    "    for name in total_name_frequencies:\n",
    "        for name2 in total_name_frequencies:\n",
    "            if name == name2[:-2]:\n",
    "                total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "                keys_to_delete.append(name2)\n",
    "            if name in name2 and name != name2:\n",
    "                total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "                keys_to_delete.append(name2)\n",
    "    for k in keys_to_delete:\n",
    "        if k in total_name_frequencies:\n",
    "            total_name_frequencies.pop(k)\n",
    "    \n",
    "    total_name_frequencies = {k:v for (k,v) in sorted(total_name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return total_name_frequencies\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not read file '../data/english_short_stories/Henry_Red_Chief.txt' using the utf-8 encoding\n",
      "'utf-8' codec can't decode byte 0xd1 in position 123: invalid continuation byte\n",
      "Could not read file '../data/english_short_stories/Henry_Red_Chief.txt' using the ascii encoding\n",
      "'ascii' codec can't decode byte 0xd1 in position 123: ordinal not in range(128)\n",
      "04/25/2023 17:56:57 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:56:57 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:56:57 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:56:57 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:56:57 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:56:58 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3c1a4882b649cc82a3cca64f445637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:56:58 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136b9a4af14040c6bcf01cad0263af2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:57:32 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:57:32 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:57:32 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:57:32 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:57:32 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:57:32 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7ec880bfcd4a7dbb1498e130a9f2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:57:32 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99a5293eedb46c4aad96456f46d68ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:57:53 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:57:53 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:57:53 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:57:53 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:57:53 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:57:54 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632f700fa4424f059919077ae207d10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:57:54 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee5fa15c3c44c6a964c24d8917ed3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not read file '../data/english_short_stories/Hills Like White Elephants.txt' using the utf-8 encoding\n",
      "'utf-8' codec can't decode byte 0xd1 in position 2447: invalid continuation byte\n",
      "Could not read file '../data/english_short_stories/Hills Like White Elephants.txt' using the ascii encoding\n",
      "'ascii' codec can't decode byte 0xd1 in position 2447: ordinal not in range(128)\n",
      "04/25/2023 17:58:21 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:58:21 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:58:21 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:58:21 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:58:21 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:58:21 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763c9cc071b243f6a25d2c8ed8f17563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:58:21 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0b5dbb277b4934b8ed566fec056ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:58:57 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:58:57 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:58:57 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:58:57 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:58:57 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:58:59 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeee916787c44d9db39bcad60c9530d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:58:59 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3284c2f7e37d47aabaee2224dade4a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not read file '../data/english_short_stories/the_gift_of_the_magi_0_Henry.txt' using the utf-8 encoding\n",
      "'utf-8' codec can't decode byte 0xd2 in position 682: invalid continuation byte\n",
      "Could not read file '../data/english_short_stories/the_gift_of_the_magi_0_Henry.txt' using the ascii encoding\n",
      "'ascii' codec can't decode byte 0xd2 in position 682: ordinal not in range(128)\n",
      "04/25/2023 17:59:43 - INFO - \t missing_keys: []\n",
      "04/25/2023 17:59:43 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 17:59:43 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 17:59:43 - INFO - \t error_msgs: []\n",
      "04/25/2023 17:59:43 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 17:59:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4aea0390bea49629cdb34e1a9d0e2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 17:59:44 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da310bf6a4a34348bedbb521c25d4419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 18:00:12 - INFO - \t missing_keys: []\n",
      "04/25/2023 18:00:12 - INFO - \t unexpected_keys: []\n",
      "04/25/2023 18:00:12 - INFO - \t mismatched_keys: []\n",
      "04/25/2023 18:00:12 - INFO - \t error_msgs: []\n",
      "04/25/2023 18:00:12 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/25/2023 18:00:14 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cad26742e84cce9abc4da1bdfe0256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/25/2023 18:00:15 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad96e7d09bca4187812b512ec190e978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from main import read_file\n",
    "import os\n",
    "\n",
    "characters_by_stories = []\n",
    "\n",
    "for f in os.listdir('../data/english_short_stories/'):\n",
    "    raw_text = read_file(f'../data/english_short_stories/{f}')\n",
    "    name_freqs = get_most_mentioned_characters(raw_text=raw_text)\n",
    "    characters_by_stories.append(name_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_data = json.dumps(characters_by_stories, indent=4)\n",
    "with open('../data/characters_by_stories.json', 'w') as outfile:\n",
    "    outfile.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
