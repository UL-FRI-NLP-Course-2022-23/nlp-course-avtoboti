{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5589177aa6e242cb98df959a3be74f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 17:50:06 INFO: Downloading default packages for language: en (English) ...\n",
      "04/24/2023 17:50:06 - INFO - \t Downloading default packages for language: en (English) ...\n",
      "2023-04-24 17:50:08 INFO: File exists: ../models/stanza_resources/en/default.zip\n",
      "04/24/2023 17:50:08 - INFO - \t File exists: ../models/stanza_resources/en/default.zip\n",
      "2023-04-24 17:50:11 INFO: Finished downloading models and saved to ../models/stanza_resources.\n",
      "04/24/2023 17:50:11 - INFO - \t Finished downloading models and saved to ../models/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "from main import read_file, get_sentences\n",
    "import stanza\n",
    "\n",
    "stanza.download(\"en\", model_dir=\"../models/stanza_resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 17:50:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "04/24/2023 17:50:11 - INFO - \t Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f70775bb24d4e379f10b424afb29293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 17:50:12 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "04/24/2023 17:50:12 - INFO - \t Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-04-24 17:50:12 INFO: Using device: cpu\n",
      "04/24/2023 17:50:12 - INFO - \t Using device: cpu\n",
      "2023-04-24 17:50:12 INFO: Loading: tokenize\n",
      "04/24/2023 17:50:12 - INFO - \t Loading: tokenize\n",
      "2023-04-24 17:50:12 INFO: Loading: pos\n",
      "04/24/2023 17:50:12 - INFO - \t Loading: pos\n",
      "2023-04-24 17:50:12 INFO: Loading: lemma\n",
      "04/24/2023 17:50:12 - INFO - \t Loading: lemma\n",
      "2023-04-24 17:50:12 INFO: Loading: ner\n",
      "04/24/2023 17:50:12 - INFO - \t Loading: ner\n",
      "2023-04-24 17:50:13 INFO: Done loading processors!\n",
      "04/24/2023 17:50:13 - INFO - \t Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "henry_red = read_file(\"../data/english_short_stories/LeiningenVstheAnts.txt\")\n",
    "\n",
    "preprocess = stanza.Pipeline(\"en\", processors=\"tokenize, ner, pos, lemma\")\n",
    "\n",
    "doc = preprocess(henry_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences(doc, lambda x: x.upos != \"PUNCT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Leiningen': 51,\n",
       " 'Carl Stephenson': 1,\n",
       " 'Hood': 1,\n",
       " 'Dullards': 1,\n",
       " 'Pell-mell': 1,\n",
       " 'Dawn': 1,\n",
       " 'Macbeth': 1,\n",
       " 'Birnam Wood': 1,\n",
       " 'Scot': 1,\n",
       " 'Dante': 1,\n",
       " 'Spry': 1,\n",
       " 'Swift': 1}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import get_named_entities\n",
    "named_entities = get_named_entities(doc)\n",
    "\n",
    "names = []\n",
    "for ne in named_entities:\n",
    "    for named_entity in ne:\n",
    "        names.append(named_entity.text)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "name_frequencies = Counter(names)\n",
    "name_frequencies = {k:v for (k,v) in sorted(name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "name_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 17:50:37 - INFO - \t missing_keys: []\n",
      "04/24/2023 17:50:37 - INFO - \t unexpected_keys: []\n",
      "04/24/2023 17:50:37 - INFO - \t mismatched_keys: []\n",
      "04/24/2023 17:50:37 - INFO - \t error_msgs: []\n",
      "04/24/2023 17:50:37 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "04/24/2023 17:50:37 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f25ed0116db413c90b586c96289cd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 17:50:38 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad596c4990954be9b85c7b4d0dfe40be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Coreference resolution\n",
    "from fastcoref import FCoref\n",
    "model = FCoref()\n",
    "# henry_red = read_file(\"../data/english_short_stories/Henry_Red_Chief.txt\")\n",
    "\n",
    "preds = model.predict(henry_red)\n",
    "clusters = preds.get_clusters(as_strings=False)\n",
    "clusters_with_names = preds.get_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 17:50:44 - INFO - \t missing_keys: []\n",
      "04/24/2023 17:50:44 - INFO - \t unexpected_keys: []\n",
      "04/24/2023 17:50:44 - INFO - \t mismatched_keys: []\n",
      "04/24/2023 17:50:44 - INFO - \t error_msgs: []\n",
      "04/24/2023 17:50:44 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastcoref.spacy_component.spacy_component.FastCorefResolver at 0x1574bb4f0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from fastcoref import spacy_component\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('fastcoref')\n",
    "\n",
    "# This is better model\n",
    "# nlp.add_pipe(\"fastcoref\", config={'model_architecture': 'LingMessCoref', 'model_path': 'biu-nlp/lingmess-coref', 'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 17:50:46 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81ff3691593466b998ece54831cc747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/24/2023 17:50:47 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bbec7a104e42b2b85f803352c75aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Leiningen': 300,\n",
       " 'Macbeth': 3,\n",
       " 'Carl Stephenson': 1,\n",
       " 'Hood': 1,\n",
       " 'Pell-mell': 1,\n",
       " 'Dawn': 1,\n",
       " 'Birnam Wood': 1,\n",
       " 'Dante': 1,\n",
       " 'Spry': 1,\n",
       " 'Swift': 1}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# henry_red = read_file(\"../data/english_short_stories/Henry_Red_Chief.txt\")\n",
    "\n",
    "# This substitutes pronouns with appropriate Named entities\n",
    "coref_doc = nlp(henry_red, component_cfg={'fastcoref': {'resolve_text': True}})\n",
    "resolved_text = coref_doc._.resolved_text\n",
    "\n",
    "# Again perform NER on new text to get all the positions of the NE.\n",
    "ne_doc = preprocess(resolved_text)\n",
    "corrected_sentences = get_sentences(ne_doc, lambda x: x.upos != \"PUNCT\")\n",
    "total_named_entities = get_named_entities(ne_doc)\n",
    "total_names = []\n",
    "for ne in total_named_entities:\n",
    "    for named_entity in ne:\n",
    "        total_names.append(named_entity.text)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "total_name_frequencies = Counter(total_names)\n",
    "total_name_frequencies = {k:v for (k,v) in sorted(total_name_frequencies.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "keys_to_delete = []\n",
    "for name in total_name_frequencies:\n",
    "    for name2 in total_name_frequencies:\n",
    "        if name == name2[:-2]:\n",
    "            total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "            keys_to_delete.append(name2)\n",
    "        if name in name2 and name != name2:\n",
    "            total_name_frequencies[name] += total_name_frequencies[name2]\n",
    "            keys_to_delete.append(name2)\n",
    "for k in keys_to_delete:\n",
    "    if k in total_name_frequencies:\n",
    "        total_name_frequencies.pop(k) \n",
    "total_name_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
